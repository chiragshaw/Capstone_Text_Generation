{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "669f8b5a486c4158ae12f4b99c2b6ec4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9e68fa42d47e410c872a5611259b884c",
              "IPY_MODEL_bc47651ea8714a5da8f46870657a3877",
              "IPY_MODEL_c9e5ec68a25f41759d5b47d6718d44c0",
              "IPY_MODEL_9e02ead75e9a433791e86a2ec07ceca0"
            ],
            "layout": "IPY_MODEL_00be7e9853524cba907263f50f987246"
          }
        },
        "39e29f3e7e454286931ab6e68f112600": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_beae1fc58fd54b5e9d40624a8580e23b",
            "placeholder": "​",
            "style": "IPY_MODEL_f1dca3fa7ea14987b401ed19940f5fe0",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "3f7463b6e7364f3080b36bb78c9d1864": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_aea0e583d26c48589e90d5dd1ff038bb",
            "placeholder": "​",
            "style": "IPY_MODEL_b91ab14fc8484f9aae3def112aba4e20",
            "value": ""
          }
        },
        "7a2051aab07a4ca7800cad875082bed1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_c5c7323dee6a474a87ab7df8f4382cf0",
            "style": "IPY_MODEL_ee57a1a010e64d4bad0778078413b058",
            "value": true
          }
        },
        "29bda1698f194e62b675a6ba5d83aa34": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_93ef01edfd5d4ca39a42a4f820e5b1b6",
            "style": "IPY_MODEL_3b0f72d45e4d41d29f059bb77ef16a54",
            "tooltip": ""
          }
        },
        "c19779896e5d4df293a45980ab828fe3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b582462d27f5471f9687453dc34870e7",
            "placeholder": "​",
            "style": "IPY_MODEL_944436f4704a4a76a919e26831325979",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "00be7e9853524cba907263f50f987246": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "beae1fc58fd54b5e9d40624a8580e23b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f1dca3fa7ea14987b401ed19940f5fe0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "aea0e583d26c48589e90d5dd1ff038bb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b91ab14fc8484f9aae3def112aba4e20": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c5c7323dee6a474a87ab7df8f4382cf0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ee57a1a010e64d4bad0778078413b058": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "93ef01edfd5d4ca39a42a4f820e5b1b6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3b0f72d45e4d41d29f059bb77ef16a54": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "b582462d27f5471f9687453dc34870e7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "944436f4704a4a76a919e26831325979": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d85bce0512b34b1e98e869c3cb5f0b94": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_db369800e99c4047a8335afdf30c8861",
            "placeholder": "​",
            "style": "IPY_MODEL_0b99410970b6419296b1a00e9833b3fd",
            "value": "Connecting..."
          }
        },
        "db369800e99c4047a8335afdf30c8861": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0b99410970b6419296b1a00e9833b3fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9e68fa42d47e410c872a5611259b884c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d5ef3d0d397941cf8ea4e65057d9d949",
            "placeholder": "​",
            "style": "IPY_MODEL_e6c697f1d5344b0c9bc5071de26268c5",
            "value": "Token is valid (permission: fineGrained)."
          }
        },
        "bc47651ea8714a5da8f46870657a3877": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f78c65fe016541a8aec700fbf57617c4",
            "placeholder": "​",
            "style": "IPY_MODEL_918e1fc072cb48fc9a953667c8fa8de2",
            "value": "Your token has been saved in your configured git credential helpers (store)."
          }
        },
        "c9e5ec68a25f41759d5b47d6718d44c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_90626e92a5764769920b325e58c73fb2",
            "placeholder": "​",
            "style": "IPY_MODEL_a2619f2d9d6f4b4bb3b634a14914909a",
            "value": "Your token has been saved to /root/.cache/huggingface/token"
          }
        },
        "9e02ead75e9a433791e86a2ec07ceca0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3d03035cefb3471db79886efa04af049",
            "placeholder": "​",
            "style": "IPY_MODEL_467fc0bca382446fa565a2315df1bfaa",
            "value": "Login successful"
          }
        },
        "d5ef3d0d397941cf8ea4e65057d9d949": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e6c697f1d5344b0c9bc5071de26268c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f78c65fe016541a8aec700fbf57617c4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "918e1fc072cb48fc9a953667c8fa8de2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "90626e92a5764769920b325e58c73fb2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a2619f2d9d6f4b4bb3b634a14914909a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3d03035cefb3471db79886efa04af049": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "467fc0bca382446fa565a2315df1bfaa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145,
          "referenced_widgets": [
            "669f8b5a486c4158ae12f4b99c2b6ec4",
            "39e29f3e7e454286931ab6e68f112600",
            "3f7463b6e7364f3080b36bb78c9d1864",
            "7a2051aab07a4ca7800cad875082bed1",
            "29bda1698f194e62b675a6ba5d83aa34",
            "c19779896e5d4df293a45980ab828fe3",
            "00be7e9853524cba907263f50f987246",
            "beae1fc58fd54b5e9d40624a8580e23b",
            "f1dca3fa7ea14987b401ed19940f5fe0",
            "aea0e583d26c48589e90d5dd1ff038bb",
            "b91ab14fc8484f9aae3def112aba4e20",
            "c5c7323dee6a474a87ab7df8f4382cf0",
            "ee57a1a010e64d4bad0778078413b058",
            "93ef01edfd5d4ca39a42a4f820e5b1b6",
            "3b0f72d45e4d41d29f059bb77ef16a54",
            "b582462d27f5471f9687453dc34870e7",
            "944436f4704a4a76a919e26831325979",
            "d85bce0512b34b1e98e869c3cb5f0b94",
            "db369800e99c4047a8335afdf30c8861",
            "0b99410970b6419296b1a00e9833b3fd",
            "9e68fa42d47e410c872a5611259b884c",
            "bc47651ea8714a5da8f46870657a3877",
            "c9e5ec68a25f41759d5b47d6718d44c0",
            "9e02ead75e9a433791e86a2ec07ceca0",
            "d5ef3d0d397941cf8ea4e65057d9d949",
            "e6c697f1d5344b0c9bc5071de26268c5",
            "f78c65fe016541a8aec700fbf57617c4",
            "918e1fc072cb48fc9a953667c8fa8de2",
            "90626e92a5764769920b325e58c73fb2",
            "a2619f2d9d6f4b4bb3b634a14914909a",
            "3d03035cefb3471db79886efa04af049",
            "467fc0bca382446fa565a2315df1bfaa"
          ]
        },
        "id": "5xz2hWJo9bcc",
        "outputId": "fc973f04-08ce-46f2-c124-07417c6db6c5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "669f8b5a486c4158ae12f4b99c2b6ec4"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "I-KiOQCBnh4r"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Greedy Search"
      ],
      "metadata": {
        "id": "6dBsMMpLRkJJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def greedy_search(prompt, model_name='openai-community/gpt2-large', max_length=200, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
        "    # Load model and tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
        "\n",
        "    # Encode the prompt to get input ids and create attention masks\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
        "    attention_mask = torch.ones_like(input_ids).to(device)\n",
        "\n",
        "    # Generate text using greedy search\n",
        "    output = model.generate(\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_mask,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "        max_length=max_length,\n",
        "        do_sample=False  # Greedy decoding\n",
        "    )\n",
        "\n",
        "    # Decode and return the generated text\n",
        "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    return generated_text\n",
        "\n",
        "# Define the device\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# Example usage\n",
        "prompt = input(\"Enter your prompt for greedy search: \")\n",
        "generated_text = greedy_search(prompt, device=device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G2kB7PCjRmn_",
        "outputId": "d979c37c-e98d-4ed0-e147-9253370ff571"
      },
      "execution_count": 6,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your prompt for greedy search: Is artificial intelligence a blessing or a curse?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Generated Text using Greedy Search:\", generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CULx50buRu4m",
        "outputId": "40dfc316-4c8f-4749-b599-06f9d29adebb"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Text using Greedy Search: Is artificial intelligence a blessing or a curse?\n",
            "\n",
            "The answer is both.\n",
            "\n",
            "The first is that artificial intelligence is a blessing. It's a way to make computers smarter.\n",
            "\n",
            "The second is that artificial intelligence is a curse. It's a way to make computers smarter.\n",
            "\n",
            "The first is a blessing because it means that computers can do more.\n",
            "\n",
            "The second is a curse because it means that computers can do less.\n",
            "\n",
            "The first is a blessing because it means that computers can do more.\n",
            "\n",
            "The second is a curse because it means that computers can do less.\n",
            "\n",
            "The first is a blessing because it means that computers can do more.\n",
            "\n",
            "The second is a curse because it means that computers can do less.\n",
            "\n",
            "The first is a blessing because it means that computers can do more.\n",
            "\n",
            "The second is a curse because it means that computers can do less.\n",
            "\n",
            "The first is a blessing because it means that computers can do\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Beam Search"
      ],
      "metadata": {
        "id": "Y1NOlDwHSAM7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def beam_search(prompt, model_name='openai-community/gpt2-large', num_beams=5, max_length=200, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
        "    # Load model and tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
        "\n",
        "    # Encode the prompt to get input ids and create attention masks\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
        "    attention_mask = torch.ones_like(input_ids)\n",
        "\n",
        "    # Generate text using beam search\n",
        "    output = model.generate(\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_mask,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "        max_length=max_length,\n",
        "        num_return_sequences=1,\n",
        "        num_beams=num_beams\n",
        "    )\n",
        "\n",
        "    # Decode and return the generated text\n",
        "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    return generated_text\n",
        "\n",
        "# Define the device\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# Example usage\n",
        "prompt = input(\"Enter your prompt for beam search: \")\n",
        "generated_text = beam_search(prompt, device=device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HYBxasTvSDt2",
        "outputId": "ee45a3fe-9e99-4361-dd85-f205a7e70e51"
      },
      "execution_count": 8,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your prompt for beam search: Is artificial intelligence a blessing or a curse?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Generated Text using Beam Search:\", generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C97qlgqQSPu_",
        "outputId": "bab6f424-1543-436b-d38a-15d8bfef225e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Text using Beam Search: Is artificial intelligence a blessing or a curse?\n",
            "\n",
            "I think it's a blessing.\n",
            "\n",
            "I think it's a curse.\n",
            "\n",
            "I think it's a blessing.\n",
            "\n",
            "I think it's a curse.\n",
            "\n",
            "I think it's a blessing.\n",
            "\n",
            "I think it's a curse.\n",
            "\n",
            "I think it's a blessing.\n",
            "\n",
            "I think it's a curse.\n",
            "\n",
            "I think it's a blessing.\n",
            "\n",
            "I think it's a curse.\n",
            "\n",
            "I think it's a curse.\n",
            "\n",
            "I think it's a curse.\n",
            "\n",
            "I think it's a curse.\n",
            "\n",
            "I think it's a curse.\n",
            "\n",
            "I think it's a curse.\n",
            "\n",
            "I think it's a curse.\n",
            "\n",
            "I think it's a curse.\n",
            "\n",
            "I think it's a curse.\n",
            "\n",
            "I think it's a curse.\n",
            "\n",
            "I think it's a curse.\n",
            "\n",
            "I think it's a curse.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Top-K Sampling"
      ],
      "metadata": {
        "id": "kXentM_4Segw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def top_k_sampling(prompt, model_name='openai-community/gpt2-large', temperature=1.0, top_k=50, max_length=200, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
        "    # Load model and tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
        "\n",
        "    # Encode the prompt to get input ids and create attention masks\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
        "    attention_mask = torch.ones_like(input_ids)\n",
        "\n",
        "    # Generate text using Top-K sampling\n",
        "    output = model.generate(\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_mask,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "        max_length=max_length,\n",
        "        num_return_sequences=1,\n",
        "        do_sample=True,\n",
        "        temperature=temperature,\n",
        "        top_k=top_k\n",
        "    )\n",
        "\n",
        "    # Decode and return the generated text\n",
        "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    return generated_text\n",
        "\n",
        "# Define the device\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# Example usage\n",
        "prompt = input(\"Enter your prompt for Top-K sampling: \")\n",
        "generated_text = top_k_sampling(prompt, device=device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8cvF4bXDShq0",
        "outputId": "b6bb5007-51f9-428c-e3fd-cc3d3be42901"
      },
      "execution_count": 10,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your prompt for Top-K sampling: Is artificial intelligence a blessing or a curse?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Generated Text using Top-K Sampling:\", generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gs02TH8DSj6S",
        "outputId": "cf82fd9b-b69c-40ad-8906-e62f9723c7ec"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Text using Top-K Sampling: Is artificial intelligence a blessing or a curse? Here are some experts' reasons why. (Jason Aldag/The Washington Post)\n",
            "\n",
            "This is not an indictment of AI or the underlying technology. I have great respect for Dr. Larry Stone and his team in building DeepMind. (When you get right down to it, AI itself is a blessing.) I am just trying to understand why so much hype has been created around this technology. I have no doubt that it can be used for great things. But, at the same time, I'm not totally convinced that we will use it in the way that its proponents envision using it.\n",
            "\n",
            "To begin with, I'd like to offer an optimistic approach. Because for many people, the idea of using AI in education — whether it's teaching people a new language or helping them get jobs in their field — is just \"too scary\" to think about. But it's not that scary. There is some evidence (e.g.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Top-P Sampling"
      ],
      "metadata": {
        "id": "GBfLCwkTq6BD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def top_p_sampling(prompt, model_name='openai-community/gpt2-large', temperature=1.0, top_p=0.9, max_length=200, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
        "    # Load model and tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
        "\n",
        "    # Encode the prompt to get input ids and create attention masks\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
        "    attention_mask = torch.ones_like(input_ids)\n",
        "\n",
        "    # Generate text using Top-P (nucleus) sampling\n",
        "    output = model.generate(\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_mask,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "        max_length=max_length,\n",
        "        num_return_sequences=1,\n",
        "        do_sample=True,\n",
        "        temperature=temperature,\n",
        "        top_p=top_p\n",
        "    )\n",
        "\n",
        "    # Decode and return the generated text\n",
        "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    return generated_text\n",
        "\n",
        "# Define the device\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# Example usage\n",
        "prompt = input(\"Enter your prompt for Top-P sampling: \")\n",
        "generated_text = top_p_sampling(prompt, device=device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "przW6NjQq9nS",
        "outputId": "1940b17a-4876-4cd4-bf8b-0e2532581424"
      },
      "execution_count": 12,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your prompt for Top-P sampling: Is artificial intelligence a blessing or a curse?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Generated Text using Top-P Sampling:\", generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GNaAf2jPq_hk",
        "outputId": "e7f21ae3-c739-4a91-e9bf-aebf977cc850"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Text using Top-P Sampling: Is artificial intelligence a blessing or a curse?\n",
            "\n",
            "\"The biggest risk is that people get excited about a technology, but then they go wrong and use it too much. The thing about artificial intelligence is that the world is so interconnected. If something like that were to happen, we could lose the ability to communicate with one another. And the danger is that people who have AI, which might be a lot of the population in the future, will forget about us, and that's a very big problem.\"\n",
            "\n",
            "Does your AI project involve human beings?\n",
            "\n",
            "\"It's probably not the first time that we're going to need to work together. We want to have this ability to be in conversation with one another as a human. It's a question of how we manage and ensure that people can communicate effectively in this new era of communication.\"\n",
            "\n",
            "Are you a robot?\n",
            "\n",
            "\"No. I'm not a robot, but a person of the mind.\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Contrastive Search"
      ],
      "metadata": {
        "id": "KIoFLbKGO9g_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def contrastive_search(prompt, model_name='openai-community/gpt2-large', penalty_alpha=0.9, max_length=200, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
        "    # Load model and tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
        "\n",
        "    # Encode the prompt to get input ids and create attention masks\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
        "    attention_mask = torch.ones_like(input_ids)\n",
        "\n",
        "    # Generate text using contrastive search\n",
        "    output = model.generate(\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_mask,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "        penalty_alpha=penalty_alpha,\n",
        "        num_return_sequences=2,\n",
        "        do_sample=True,\n",
        "        max_length=max_length\n",
        "    )\n",
        "\n",
        "    # Decode and return the generated text\n",
        "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    return generated_text\n",
        "\n",
        "# Define the device\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# Example usage\n",
        "prompt = input(\"Enter your prompt for contrastive search: \")\n",
        "generated_text = contrastive_search(prompt, device=device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eLZfPbokPA_S",
        "outputId": "3736713c-2f1c-4c48-f843-8448eb97c408"
      },
      "execution_count": 40,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your prompt for contrastive search: Is artificial intelligence a blessing or a curse?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Generated Text using Contrastive Search:\", generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hgcxHtBRPF67",
        "outputId": "e892d13d-5042-40a8-cd4b-5b3225bf92c8"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Text using Contrastive Search: Is artificial intelligence a blessing or a curse?\n",
            "\n",
            "A blessing or a curse?\n",
            "\n",
            "A bit of a curse. On the one hand, the future of intelligence research is going to be guided very consciously by the very big \"intelligence\" machines that have been on our planet for the last 50 years. By 2050, there ought to be a whole other set of machines that we haven't discovered yet. There is even the possibility that they won't be even there yet. (I'm talking about the \"intelligence explosion\" that's been under way for the last few decades.)\n",
            "\n",
            "By then, we'll know a lot about the intelligence of such machines. And, on the other hand, we know that the ability to make any new intelligence is finite in nature—that it will eventually get destroyed. So let's say that we're lucky: we don't live in the next phase of the intelligence explosion. But what will happen then?\n",
            "\n",
            "One possibility is that we'll discover\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Locally Typical"
      ],
      "metadata": {
        "id": "io_rtBWSCAfT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "CNiMNPsea7kW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "076e485d-1b98-45a2-ecb5-b46d6a791ac0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your prompt for locally typical sampling: Is artificial intelligence a blessing or a curse?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        }
      ],
      "source": [
        "def locally_typical_sampling_search(prompt, model_name='openai-community/gpt2-large', max_length=200, top_k=40, typical_p=0.95, no_repeat_ngram_size=2, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
        "    # Load model and tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_name, use_auth_token=True).to(device)\n",
        "\n",
        "    # Encode the prompt to get input ids and create attention masks\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
        "    attention_mask = torch.ones_like(input_ids)\n",
        "\n",
        "    # Generate text using locally typical sampling\n",
        "    output = model.generate(\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_mask,\n",
        "        max_length=max_length,\n",
        "        no_repeat_ngram_size=no_repeat_ngram_size,\n",
        "        do_sample=True,\n",
        "        top_k=top_k,\n",
        "        typical_p=typical_p\n",
        "    )\n",
        "\n",
        "    # Decode and return the generated text\n",
        "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    return generated_text\n",
        "\n",
        "# Define the device\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# Example usage\n",
        "prompt = input(\"Enter your prompt for locally typical sampling: \")\n",
        "generated_text = locally_typical_sampling_search(prompt, device=device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Generated Text using locally typical sampling:\", generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JT8_f-Bj68S3",
        "outputId": "47f3d6f7-dd1d-45c1-d31b-87e1103f80cf"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Text using locally typical sampling: Is artificial intelligence a blessing or a curse? In this case, it's both, because we can use AI to enhance the job market, and also to put an end to it.\"\n",
            "\n",
            "While most economists believe that a rise in AI could cause widespread job loss, Dr. Karpeles believes that this is unlikely: \"The current growth in the use of AI is positive. There's an increasing sense of empowerment and excitement. This is a positive development. It's more likely that the AI technology will be used to augment humans,\" he says.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Speculative Sampling"
      ],
      "metadata": {
        "id": "dNZJCe1rTvHL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def speculative_decoding(prompt, draft_model_name='openai-community/gpt2-medium', refined_model_name='openai-community/gpt2-large', alpha=0.9, max_length=200, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
        "    # Load draft model and tokenizer\n",
        "    draft_tokenizer = AutoTokenizer.from_pretrained(draft_model_name)\n",
        "    draft_model = AutoModelForCausalLM.from_pretrained(draft_model_name).to(device)\n",
        "\n",
        "    # Load refined model\n",
        "    refined_model = AutoModelForCausalLM.from_pretrained(refined_model_name).to(device)\n",
        "\n",
        "    # Encode the prompt\n",
        "    input_ids = draft_tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
        "    attention_mask = torch.ones_like(input_ids).to(device)\n",
        "\n",
        "    # Generate draft text\n",
        "    draft_output = draft_model.generate(\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_mask,\n",
        "        max_length=max_length,\n",
        "        do_sample=True,\n",
        "        temperature=1.0\n",
        "    )\n",
        "\n",
        "    # Convert draft output to tokens\n",
        "    draft_tokens = draft_output[0].tolist()  # Convert to list for easier manipulation\n",
        "\n",
        "    # Refine the draft using the refined model\n",
        "    refined_tokens = draft_tokens.copy()\n",
        "    for _ in range(len(draft_tokens), max_length):\n",
        "        # Prepare input for refined model\n",
        "        refined_input_ids = torch.tensor([refined_tokens], device=device)\n",
        "\n",
        "        # Get logits from the refined model\n",
        "        refined_output = refined_model(refined_input_ids)\n",
        "        logits = refined_output.logits[:, -1, :]\n",
        "        refined_probs = torch.softmax(logits, dim=-1)\n",
        "\n",
        "        # Apply speculative decoding rule\n",
        "        if torch.rand(1).item() < alpha:\n",
        "            next_token = refined_tokens[-1]\n",
        "        else:\n",
        "            next_token = torch.multinomial(refined_probs, num_samples=1).item()\n",
        "\n",
        "        refined_tokens.append(next_token)\n",
        "\n",
        "        # Update input_ids with the newly selected token\n",
        "        refined_input_ids = torch.cat([refined_input_ids, torch.tensor([[next_token]], device=device)], dim=1)\n",
        "\n",
        "    # Decode and return the refined text\n",
        "    generated_text = draft_tokenizer.decode(refined_tokens, skip_special_tokens=True)\n",
        "    return generated_text\n",
        "\n",
        "# Define the device\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# Example usage\n",
        "prompt = input(\"Enter your prompt for Speculative Decoding: \")\n",
        "generated_text = speculative_decoding(prompt, device=device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6kLr6p_MTyla",
        "outputId": "15159fda-9c35-4397-df50-fac1e09ab8db"
      },
      "execution_count": 19,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your prompt for Speculative Decoding: Is artificial intelligence a blessing or a curse?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Generated Text using Speculative Decoding:\", generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JH1vGSbST0GV",
        "outputId": "07217c0f-27f1-49f0-8884-a842e6de631f"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Text using Speculative Decoding: Is artificial intelligence a blessing or a curse?\n",
            "\n",
            "A: Artificial intelligence has great potential, yet it's far from perfect. AI researchers have learned that even when it delivers a brilliant performance on a test, what makes AI work best is its use of context. One example is the task of understanding text. What it takes to understand text is different for humans than machines. Many new applications are coming online, however, and there are still a few technical obstacles that need to be overcome for AI to achieve its promise. And yet, we're seeing advances, in areas from speech recognition to image processing.\n",
            "\n",
            "A second category involves AI technology for medical diagnostics and diagnostics, and it's just entering the public domain. AI could soon transform the way we diagnose, diagnose, diagnose. The field of health-care diagnostics for medical use has many potential benefits, from diagnosing infections in the airways to controlling muscle spasms or treating disease when the nerves are injured. But how are\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Mirostat"
      ],
      "metadata": {
        "id": "lGdMiiz1T7p1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mirostat_decoding(prompt, model_name='openai-community/gpt2-large', target_ppl=1.0, eta=0.1, max_length=200, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
        "    # Load model and tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
        "\n",
        "    # Encode the prompt\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
        "    attention_mask = torch.ones_like(input_ids).to(device)\n",
        "\n",
        "    # Initialize parameters\n",
        "    generated_tokens = []\n",
        "    current_ppl = target_ppl\n",
        "    tau = 5.0  # A constant parameter that controls adjustments\n",
        "    k = 30  # Initial top-k sampling value\n",
        "    vocab_size = model.config.vocab_size\n",
        "\n",
        "    # Mirostat loop\n",
        "    for _ in range(max_length):\n",
        "        # Get the model's logits\n",
        "        with torch.no_grad():\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        logits = outputs.logits\n",
        "        probs = torch.softmax(logits[:, -1, :], dim=-1)\n",
        "\n",
        "        # Adjust k to be within valid range\n",
        "        k = min(max(1, k), vocab_size)\n",
        "\n",
        "        # Apply Top-k sampling\n",
        "        top_k_probs, top_k_indices = torch.topk(probs, k=k, dim=-1)\n",
        "        next_token = torch.multinomial(top_k_probs[0], num_samples=1)\n",
        "        generated_tokens.append(top_k_indices[0, next_token].item())\n",
        "\n",
        "        # Adjust perplexity dynamically\n",
        "        current_token_log_prob = torch.log(top_k_probs[0, next_token].float()).item()\n",
        "        delta = current_token_log_prob + torch.log(torch.tensor([target_ppl], dtype=torch.float))\n",
        "        ppl_adjustment = tau * (delta / (eta + delta.abs()))\n",
        "        current_ppl *= torch.exp(-ppl_adjustment).item()\n",
        "\n",
        "        # Safeguard against excessively large k values\n",
        "        if current_ppl > 1e6:  # Threshold to prevent overflow\n",
        "            k = vocab_size\n",
        "        else:\n",
        "            k = max(1, int(k * current_ppl / target_ppl))\n",
        "\n",
        "        # Update input ids and attention mask\n",
        "        input_ids = torch.cat([input_ids, torch.tensor([[generated_tokens[-1]]], device=device)], dim=1)\n",
        "        attention_mask = torch.cat([attention_mask, torch.ones((1, 1), device=device)], dim=1)\n",
        "\n",
        "    # Decode and return the generated text\n",
        "    generated_text = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
        "    return generated_text\n",
        "\n",
        "# Define the device\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# Example usage\n",
        "prompt = input(\"Enter your prompt for Mirostat: \")\n",
        "generated_text = mirostat_decoding(prompt, device=device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GTMbOhatT9ii",
        "outputId": "208695f4-ccad-4a54-d731-69a4d8239f39"
      },
      "execution_count": 46,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your prompt for Mirostat: Is artificial intelligence a blessing or a curse?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Generated Text using Mirostat Decoding:\", generated_text)"
      ],
      "metadata": {
        "id": "ysbJdSpkUAnm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05762de0-e66f-493e-8ad5-f839047e7a66"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Text using Mirostat Decoding:  If a human wants to become a space-crow man, they definitely want to look what his course of action looks like. Aside from clothes and personal care, he would inspect his Pup dies before they're eaten, so he can love using them as granola bars and other edible snacks for a human being. Pretty neat!\n",
            "\n",
            "Whether artificial intelligence could enact human emotions remains to be seen, but he's not immune to them either. A well-intentioned human could make a Pup come to life and go on adventures with an attendant, drawing their soul out from their body when they pray. They would look cute with a kitten, p quickly studying them before they vanish. When Night leaves to mingle with the all-terrestrial tablets, he brings his smart-ass roommate with him.\n",
            "\n",
            "BoJack Horseman isn't quite a joke, but neither is it naïve ironic. The mascot uploading updates viewed from 30 years hence into the future doesn't quite wear the same costume\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Context-Aware Decoding"
      ],
      "metadata": {
        "id": "lbE5pppfuI74"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def context_aware_decoding(prompt, model_name='openai-community/gpt2-large', max_length=200, context_window_size=5, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
        "    # Load model and tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
        "\n",
        "    # Encode the prompt to get input ids and create attention masks\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
        "    attention_mask = torch.ones_like(input_ids)\n",
        "\n",
        "    # Generate text with context awareness by trimming input_ids if necessary\n",
        "    if input_ids.size(1) > context_window_size:\n",
        "        input_ids = input_ids[:, -context_window_size:]\n",
        "        attention_mask = attention_mask[:, -context_window_size:]\n",
        "\n",
        "    # Generate text\n",
        "    output = model.generate(\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_mask,\n",
        "        max_length=max_length,\n",
        "        do_sample=True,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "        top_k=50,  # Adjust this parameter for diverse outputs\n",
        "    )\n",
        "\n",
        "    # Decode and return the generated text\n",
        "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    return generated_text\n",
        "\n",
        "# Define the device\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# Example usage\n",
        "prompt = input(\"Enter your prompt for Context-Aware Decoding: \")\n",
        "generated_text = context_aware_decoding(prompt, device=device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rqT9mHsWuhoj",
        "outputId": "21835f43-5b3d-42bd-d7b5-9c866af005f0"
      },
      "execution_count": 48,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your prompt for Context-Aware Decoding: Is artificial intelligence a blessing or a curse?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Generated Text using Context-Aware Decoding:\", generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yVzj5Wm6uoLT",
        "outputId": "ce35fa99-210e-4c67-86a8-935d9b29f292"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Text using Context-Aware Decoding:  blessing or a curse? The first is an accident of evolution, producing one individual with certain biological properties that help it survive. Then comes the second: \"The curse comes from being genetically inclined to reproduce and giving one's offspring advantages in the genes of other species.\"\n",
            "\n",
            "There are a whole host of caveats to this. The most prominent of these is this: \"Curse or blessings come from how one treats one's offspring, not from their natural state. In this regard you cannot avoid using genetic science as a tool for morality.\"\n",
            "\n",
            "This argument goes back to the ancient Greeks, who believed they had reason to be happy in spite of their environment, and also to the medieval Church, who promoted procreation, even when it harmed others. But some scientists are less inclined to adopt such a grim outlook. They argue that moral judgements are predicated not of natural outcomes but of our actions in past, or present, environments. If you have behaved badly in the past, you may\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Determinantal Point Process (DPP) Sampling"
      ],
      "metadata": {
        "id": "gJnwVRzNvda-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dpp_sampling_search(prompt, model_name='openai-community/gpt2-large', max_length=200, lambda_dpp=0.7, top_k=50, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
        "    # Load model and tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
        "\n",
        "    # Encode the prompt to get input ids and create attention masks\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
        "    attention_mask = torch.ones_like(input_ids)\n",
        "\n",
        "    # Generate multiple sequences for initial diversity using Top-K Sampling\n",
        "    outputs = model.generate(\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_mask,\n",
        "        max_length=max_length,\n",
        "        num_return_sequences=5,  # Generating multiple outputs to apply DPP\n",
        "        do_sample=True,\n",
        "        top_k=top_k,\n",
        "    )\n",
        "\n",
        "    # Apply DPP to select diverse outputs\n",
        "    diverse_outputs = apply_dpp(outputs, lambda_dpp)  # Custom function to apply DPP\n",
        "\n",
        "    # Decode and return the most diverse generated text\n",
        "    generated_text = tokenizer.decode(diverse_outputs[0], skip_special_tokens=True)\n",
        "    return generated_text\n",
        "\n",
        "def apply_dpp(outputs, lambda_dpp):\n",
        "    # Convert model outputs to a numpy array\n",
        "    outputs_np = outputs.cpu().numpy()\n",
        "\n",
        "    # Compute cosine similarity between all pairs of outputs\n",
        "    similarity_matrix = cosine_similarity(outputs_np)\n",
        "\n",
        "    # Create a kernel matrix using the similarity scores\n",
        "    kernel_matrix = lambda_dpp * similarity_matrix + (1 - lambda_dpp) * np.identity(len(outputs_np))\n",
        "\n",
        "    # Implement greedy DPP sampling to select diverse subsets\n",
        "    selected_indices = greedy_dpp_selection(kernel_matrix)\n",
        "\n",
        "    # Return the outputs corresponding to the selected indices\n",
        "    return outputs[selected_indices]\n",
        "\n",
        "def greedy_dpp_selection(kernel_matrix):\n",
        "    # Greedily select indices for DPP sampling\n",
        "    n = kernel_matrix.shape[0]\n",
        "    selected = []\n",
        "    remaining = list(range(n))\n",
        "\n",
        "    for _ in range(n):\n",
        "        # Select the index with the maximum marginal gain\n",
        "        max_gain_index = max(remaining, key=lambda i: np.linalg.det(kernel_matrix[selected + [i], :][:, selected + [i]]))\n",
        "        selected.append(max_gain_index)\n",
        "        remaining.remove(max_gain_index)\n",
        "\n",
        "    return selected\n",
        "\n",
        "# Example usage\n",
        "prompt = input(\"Enter your prompt for DPP: \")\n",
        "generated_text = dpp_sampling_search(prompt, device=device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jQRVbDZDviAY",
        "outputId": "002ec8c7-8c89-4d94-fd70-7168ed08d6b3"
      },
      "execution_count": 27,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your prompt for DPP: Is artificial intelligence a blessing or a curse?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Generated Text using DPP:\", generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wmOX2WwwwBNQ",
        "outputId": "59258671-035e-4023-8bb9-69b31662e6aa"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Text using DPP: Is artificial intelligence a blessing or a curse? The question in the popular consciousness is, \"Should we or shouldn't we?\" There have been a tremendous series of debates about whether we should be working on this field. People debate endlessly from a scientific perspective, from a economic perspective, from a societal perspective. In the end, we have to do what's best for our society and our nation. When our nation's defense strategy is the \"Do whatever it takes to win\" philosophy, it doesn't matter whether we understand what's at stake.\n",
            "\n",
            "And then there's the other side of the coin. Are we investing in AI because we are a superpower, because we will continue to rise or are we investing because it's a social good that's part of human nature and should be nurtured? If we continue to grow the economy, we're in a much more comfortable position. The same is true for the US and the UK. If China and other developing countries, and even we,\n"
          ]
        }
      ]
    }
  ]
}